# @package _group_

_name: top_level.debug.staircase.lm

criterion:
  _name: cross_entropy
  # _name: cross_entropy_with_ponder

common:
  _name: staircase.lm.bookcorpusopen
  no_progress_bar: true
  log_interval: 100
  seed: 1
  fp16: true
  # cpu: true

model:
  _name: staircase_lm
  num_bottom_layers: 10
  num_staircase_layers: 0
  num_top_layers: 0
  share_decoder_input_output_embed: true
  position_encoding: xpos
  decoder:
      input_dim: 640
      output_dim: ${model.decoder.embed_dim}
      embed_dim: 640
      ffn_embed_dim: 2560
      attention_heads: 10
      xformers_att_config: '{"name":"scaled_dot_product"}'
      learned_pos: true


task:
  _name: language_modeling
  tokens_per_sample: 200
  sample_break_mode: none
  shorten_method: truncate
  dataset_impl: mmap

dataset:
  skip_invalid_size_inputs_valid_test: false
  required_batch_size_multiple: 1
  validate_interval: 5_000
  # we need this since our validation set is 1/4 of the entire corpus
  # with 20k tokens eff. bsz this means 570 steps ~ 1% corpus, we use 2*570=1140 or ~2%
  max_valid_steps: 570
  # ----
  # # with 20k tokens eff. bsz this means 570 steps ~ 1% corpus, we use 25*570=1140 or ~25%
  # max_valid_steps: 14250
  fixed_validation_seed: 1
  batch_size: 100
  num_workers: 2

optimizer:
  _name: adam
  weight_decay: 0.01

optimization:
  update_freq:
    - 1
  lr:
    - 3e-4
    # paper used {1,3,5}"e-4"
  max_epoch: 1
  max_update: 57200

# lr_scheduler:
#   _name: cosine
#   warmup_updates: 570
#   warmup_init_lr: 1e-7
#   lr: 
#     - 3e-4
#   min_lr: 1e-5
#   lr_period_updates: 58_000

lr_scheduler:
  _name: polynomial_decay
  warmup_updates: 1

checkpoint:
  save_interval_updates: 5_000
  keep_interval_updates: 1
  keep_last_epochs: 1
  # keep_best_checkpoints: 1
  no_save: false
  no_epoch_checkpoints: false
  no_last_checkpoints: false
  save_dir: ???
  # reset_dataloader: true
  # reset_optimizer: true
  # reset_lr_scheduler: true
  # reset_meters: true

hydra:
  job:
    name: hydra.debug.staircase.lm

